rm(list=ls())
imdb_ids <- c("tt0057012", "tt0000000", "tt0083946", "tt0089881", "NOT AN IMDB ID")
urls <- paste0("http://www.imdb.com/title/", imdb_ids, "/keywords")

imdb_keywords <- function(tree) {
    nodes <- tree2node(tree, select='class="keyword"', children="a")
    keywords <- ldply(nodes, ahref)
    return(keywords)
}

fx <- imdb_keywords

format="html"
chunk_size=2
sleep=0.01
emr=FALSE
clusterObject=NULL
# gen input data
generate_input_list <- function(input_urls) {

    # generate ids to sort on
    n <- length(input_urls)
    sort <- 1:n

    # generate list of chunks of urls to scrape

    chunks <- seq(1,n,chunk_size)
    n_chunks <- length(chunks)

    list <- vector("list", n_chunks)
    for (i in 1:n_chunks) {
        if(i==n_chunks) {
            the_chunk <- chunks[i]:n
        } else {
            the_chunk <- chunks[i]:(chunks[i+1]-1)
        }
        list[[i]] <- data.frame(sort = sort[the_chunk], url = input_urls[the_chunk], stringsAsFactors=F)
    }
    return(list)
}

# create function wrapper
 run_fx <- function(x, the_fx) {
    the_function <- match.fun(the_fx)
    the_function(x)
}

# applying this function to html trees and handle errors
parse_and_handle_errors <- function(d) {
    # CONVERT HTML TO PARSEABLE TREE
    tree <- htmlTreeParse(d$html, useInternalNodes=TRUE)

    # RUN THE FUNCTION
    output <- try(run_fx(x=tree, the_fx=fx), TRUE)

    # HANDLE ERRORS
    if (class(output)=='try-error'| is.null(output) | nrow(output)==1) {
        cat("!\n")
        warning("had a problem scraping ", d$url, "\n")
        df <- data.frame(error = 1, stringsAsFactors=F)
        df$sort <- d$sort
     } else {
        df <- data.frame(output, stringsAsFactors=F)
        df$sort <- d$sort
        df$error <- 0
     }
    return(df)
}

reducer <- function(df_list, sort) {
    # order dfs from smallest number of columns to greatest number of columns
    # this will ensure that additional columns are filled in by rbind.fill
    col_list <- unlist(lapply(df_list, ncol))
    df_list <- df_list[order(col_list)]

    # reduce data
    output <- rbind.fill(df_list)

    # reorder by sort ids, discard
    if(sort) {
      output <- output[order(output$sort),]
      output$sort <- NULL
    }
    return(output)
}

#
runner <-  function(df) {

    # download the chunk of urls
    df$html <- getURL(df$url)

    # apply the parsing function to the resulting html pages
    chunk_dfs <- ddply(df, .(url), parse_and_handle_errors)

    output <- reducer(chunk_dfs, sort=FALSE)

    return(output)
}


 #___________________________________________________#

# initialize data
list <- generate_input_list(urls)
output <- llply(list, runner, .progress="text")
clean_output <- reducer(output, sort=TRUE)
# # announce scraping
# cat("now scraping", length(urls), "pages,", chunk_size, "at a time...\n")
# # start scraping and logging errors
# if(emr) {
#     if(is.null(clusterObject)) {
#         stop("must provide clusterObject initialized from segue to run on EMR")
#     }
#     cat("using emr...\n")
#     output <- emrlapply(clusterObject, list, runner)
# } else {
#     cat("using llply...\n")

# }
# cat("reducing output...\n")


